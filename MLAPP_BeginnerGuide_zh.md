# MLAPP 新手友好指南（逐章）

> 结构：为什么重要｜入门先做什么｜直觉图景｜最小例子｜常见误区｜术语小抄｜什么时候用｜读完你应该会｜往后怎么学


## 第 1 章 Introduction


### 为什么重要（一句话）
- 把“学习=在不确定性下做决策”的总框架交代清楚，决定你读后面是否不迷路。


### 入门先做什么（Skim / Focus / Skip）
- 先过一遍1.1–1.3图示，记住监督/无监督/评估三件事。
- 本章细节不用背；知道过拟合/模型选择/维数灾难这些词即可。


### 直觉图景
- 把模型想成“生成数据的故事”，学习就是反推故事参数；判别式模型像“直接学打分规则”。


### 最小例子（不写代码也能做）
- 下载一个UCI数据集（如Iris），各做一次：KNN、Logistic回归、5折交叉验证，比较训练/验证准确率。
- 画学习曲线（样本量→性能），体会过拟合/欠拟合。


### 常见误区
- 把验证集泄露到训练；只看训练误差；把‘非参数/参数’等同‘好/坏’。


### 术语小抄
- 监督学习、无监督学习、回归/分类、过拟合、交叉验证、模型选择、No Free Lunch。


### 什么时候用（实践场景）
- 任何ML项目启动前的‘问题类型→度量→基线’三连问。


### 读完你应该会
- 识别问题类型，搭建基线与评估方案。


### 往后怎么学（书内锚点）
- → Ch2 概率语言；→ Ch6 评估与风控；→ Ch7/8 建基线模型。


## 第 2 章 Probability


### 为什么重要（一句话）
- 读得懂后文公式与‘不确定性量化’的必备语法。


### 入门先做什么（Skim / Focus / Skip）
- 先会用Bayes公式和条件独立；常见分布只需记‘形状/用途’。
- 熵/KL/MI先理解含义，不必推导。


### 直觉图景
- 概率是‘相信程度’；条件独立=拆问题；信息论量化‘惊讶/差异/相关’。


### 最小例子（不写代码也能做）
- 用抛硬币数据做Beta先验→Beta后验；画先验/后验随样本量变化。
- 用KL(p||q)比较两个一维高斯，体会‘不对称’。


### 常见误区
- 把独立误当成互斥；把pdf值当概率；忽略单位/维度。


### 术语小抄
- 边缘/条件/联合、独立/条件独立、熵、KL散度、互信息、蒙特卡罗。


### 什么时候用（实践场景）
- 设计算法时的损失与先验假设；做不确定性传播与指标对比。


### 读完你应该会
- 能进行简单的后验/后验预测计算；能用MC近似期望。


### 往后怎么学（书内锚点）
- → Ch3 共轭分析；→ Ch4 高斯；→ Ch20–24 推断方法。


## 第 3 章 Generative models (discrete)


### 为什么重要（一句话）
- 教你在离散/计数数据上用共轭先验得出闭式后验，是最快见效的贝叶斯实践。


### 入门先做什么（Skim / Focus / Skip）
- 重点放在Beta–Binomial/Dirichlet–Multinomial四件套：似然/先验/后验/后验预测。
- 朴素贝叶斯作为第一个生成式分类器。


### 直觉图景
- 先验像‘软约束’，数据多了后验被数据主导；朴素贝叶斯假设特征条件独立。


### 最小例子（不写代码也能做）
- 用短信垃圾分类：CountVectorizer→MultinomialNB→准确率/PR曲线；试不同平滑α。
- 计算词汇互信息，筛5–10个高辨识词。


### 常见误区
- 把拉普拉斯平滑忘掉；词袋稀疏向量未做log-sum-exp数值稳定。


### 术语小抄
- Beta/Dirichlet、后验预测、朴素贝叶斯、互信息、log-sum-exp。


### 什么时候用（实践场景）
- 文本分类、A/B计数数据、点击/转化率Beta-Binomial估计。


### 读完你应该会
- 手算小规模后验；搭建NB文本分类基线。


### 往后怎么学（书内锚点）
- → Ch5 模型比较；→ Ch27 LDA与离散潜变量。


## 第 4 章 Gaussian models


### 为什么重要（一句话）
- 高斯家族解析友好，是线模/卡尔曼/GP的共同语。


### 入门先做什么（Skim / Focus / Skip）
- 先会多元高斯‘条件/边缘仍然高斯’及其公式；理解LDA/QDA区别。


### 直觉图景
- 协方差矩阵描述‘拉伸/旋转’；精度矩阵稀疏=条件独立。


### 最小例子（不写代码也能做）
- 对两类二维数据拟合LDA与QDA，画决策边界；在小样本下加入协方差收缩看稳定性提升。


### 常见误区
- 协方差矩阵奇异；类协方差估计不稳定导致过拟合。


### 术语小抄
- MVN、LDA/QDA、Wishart/Inv-Wishart、信息形式。


### 什么时候用（实践场景）
- 连续特征分类/回归、传感器融合、异常检测（马氏距离）。


### 读完你应该会
- 推导条件高斯；实现LDA基线并做正则化。


### 往后怎么学（书内锚点）
- → Ch7 线性回归；→ Ch15 GP；→ Ch18 Kalman。


## 第 5 章 Bayesian statistics


### 为什么重要（一句话）
- 把‘不确定性’放进参数与预测，并提供模型比较的系统方法。


### 入门先做什么（Skim / Focus / Skip）
- 理解证据（边际似然）与Bayes因子；会画/读可信区间；知道层次贝叶斯动机。


### 直觉图景
- 证据自动惩罚复杂度=Occam剃刀；层次先验在多任务共享统计强度。


### 最小例子（不写代码也能做）
- Beta-Binomial两组转化率比较：后验差异分布与P(差>0)。
- 做一次简单层次模型（如多城市转化率）看缩尾效应。


### 常见误区
- 把可信区间误读为频率意义；把非信息先验当‘没有先验’。


### 术语小抄
- MAP/后验均值、证据、Bayes因子、层次贝叶斯、经验贝叶斯、Bayes决策。


### 什么时候用（实践场景）
- 小样本估计、AB测试、风险敏感阈值选择。


### 读完你应该会
- 计算/解释后验与证据；设计成本敏感决策。


### 往后怎么学（书内锚点）
- → Ch21–24 近似/采样；→ Ch7–9 GLM贝叶斯版。


## 第 6 章 Frequentist statistics


### 为什么重要（一句话）
- 教你如何可靠评估与选择模型，是工程落地的护城河。


### 入门先做什么（Skim / Focus / Skip）
- 偏差-方差、ERM/SRM、交叉验证；会Bootstrap的用法。


### 直觉图景
- 泛化来自容量控制与独立验证；正则化等价于加入先验的频率派版。


### 最小例子（不写代码也能做）
- 同一数据上比较K折CV vs 留出法；画偏差-方差示意（模型复杂度→误差）。


### 常见误区
- 数据泄露；重复使用验证集调参造成乐观偏差。


### 术语小抄
- 一致性/无偏/最小方差、CV、VC界、替代损失。


### 什么时候用（实践场景）
- 模型比较、特征选择、风险评估与报告。


### 读完你应该会
- 搭建规范评估流水线；理解统计学习上界的含义。


### 往后怎么学（书内锚点）
- → Ch8/14 优化/间隔；→ Ch16 集成。


## 第 7 章 Linear regression


### 为什么重要（一句话）
- 监督学习最重要的起点与可解释基线。


### 入门先做什么（Skim / Focus / Skip）
- 从几何视角理解OLS；会岭回归；知道何时用稳健回归。


### 直觉图景
- 最小二乘=投影；岭回归=缩小系数换泛化。


### 最小例子（不写代码也能做）
- 同一数据集做OLS/岭/稳健回归，比较系数与误差；用证据法/CV选λ。


### 常见误区
- 多重共线性导致方差爆炸；以R^2衡量一切。


### 术语小抄
- OLS、岭回归、证据最大化（EB）。


### 什么时候用（实践场景）
- 数值型目标预测、因果前的相关建模、解释性需求强的场景。


### 读完你应该会
- 稳定地拟合与诊断线模；选择正则强度。


### 往后怎么学（书内锚点）
- → Ch13 L1稀疏；→ Ch15 GP对比。


## 第 8 章 Logistic regression


### 为什么重要（一句话）
- 工程分类首选基线，凸优化稳定，解释清晰。


### 入门先做什么（Skim / Focus / Skip）
- 掌握Newton/IRLS或LBFGS；L2正则与多类softmax；会做在线SGD基线。


### 直觉图景
- 对数几率=线性函数；凸带来全局最优。


### 最小例子（不写代码也能做）
- MNIST子集：softmax回归 + L2 + 提前停止；与SVM对比。


### 常见误区
- 不做标准化；类别极不平衡却只看Accuracy。


### 术语小抄
- 对数似然、IRLS、正则、softmax、多类。


### 什么时候用（实践场景）
- CTR/风控/文本分类等大路场景。


### 读完你应该会
- 稳定训练二/多类；做阈值调整以平衡PR。


### 往后怎么学（书内锚点）
- → Ch9 GLM统一；→ Ch14 SVM对比。


## 第 9 章 GLMs & Exponential family


### 为什么重要（一句话）
- 把回归/分类放进统一语言，处理二项/泊松/序数等多种输出。


### 入门先做什么（Skim / Focus / Skip）
- 理解指数族与链接函数；做过Poisson/Probit的小例子。


### 直觉图景
- 充分统计量让估计高效；链接把线性预测映到目标域。


### 最小例子（不写代码也能做）
- 用Poisson回归建模每日事件次数；比较Logit vs Probit分类。


### 常见误区
- 选择错误的方差函数；忽略过度离散。


### 术语小抄
- 指数族、链接、对数配分函数、GLMM。


### 什么时候用（实践场景）
- 计数/比例/序数/多任务学习。


### 读完你应该会
- 为多种输出挑选合适GLM并拟合。


### 往后怎么学（书内锚点）
- → Ch21 近似推断；→ Ch15 GP–GLM。


## 第 10 章 Directed graphical models


### 为什么重要（一句话）
- 学会用图表达条件独立与因果假设，组织复杂模型。


### 入门先做什么（Skim / Focus / Skip）
- 会d-分离、Markov blanket、BayesBall；理解板式记号。


### 直觉图景
- 图像“因果流程图”，因独立性可分解计算。


### 最小例子（不写代码也能做）
- 构造一个诊断小网：症状→疾病→检测；用条件独立简化推断。


### 常见误区
- 把相关当因果；忽略混杂路径（collider）。


### 术语小抄
- DAG、d-分离、Markov blanket、板式记号。


### 什么时候用（实践场景）
- 医学诊断、推荐理由建模、因果假设表达。


### 读完你应该会
- 读/画Bayes网，写出因子分解式。


### 往后怎么学（书内锚点）
- → Ch11 EM；→ Ch20 精确推断；→ Ch26 结构学习。


## 第 11 章 Mixture models & EM


### 为什么重要（一句话）
- 处理多模态/簇结构/专家分解的万能工具。


### 入门先做什么（Skim / Focus / Skip）
- 掌握EM思路与下界；学会GMM/专家混合，理解初始化影响。


### 直觉图景
- E步填‘隐标签’，M步做‘加权极大化’。


### 最小例子（不写代码也能做）
- 用GMM聚类二维数据；改不同K与初始化，观察局部最优。


### 常见误区
- 把聚类当真标签；忽视不识别性与成分置换问题。


### 术语小抄
- 混合模型、E/M步、ELBO、Baum–Welch。


### 什么时候用（实践场景）
- 密度估计、无监督聚类、分段专家。


### 读完你应该会
- 实现GMM-EM；用BIC/交叉验证选K。


### 往后怎么学（书内锚点）
- → Ch12 潜线性；→ Ch25 聚类。


## 第 12 章 Latent linear models (FA/PCA/ICA)


### 为什么重要（一句话）
- 降维/可视化/去噪/源分离不可或缺。


### 入门先做什么（Skim / Focus / Skip）
- PCA↔SVD；PPCA/FA给概率解释；FastICA做非高斯独立成分。


### 直觉图景
- 低秩=把数据投到少数方向；ICA靠非高斯把信号拆开。


### 最小例子（不写代码也能做）
- PCA压缩人脸；重构误差与主成分数关系；做一次ICA音源分离。


### 常见误区
- 把方差解释率当泛化指标；忘标准化。


### 术语小抄
- SVD、PPCA/FA、ICA、CCA/PLS。


### 什么时候用（实践场景）
- 特征压缩、可视化、盲源分离。


### 读完你应该会
- 选维度并解释主成分；用EM拟合FA/PPCA。


### 往后怎么学（书内锚点）
- → Ch27 主题模型类比；→ Ch13 稀疏扩展。


## 第 13 章 Sparse linear models


### 为什么重要（一句话）
- 高维小样本时代的核心方法论。


### 入门先做什么（Skim / Focus / Skip）
- 从Lasso与坐标下降开始；理解KKT条件与正则路径。


### 直觉图景
- L1像‘软阈值’把小系数推到0；ARD用先验自动压缩。


### 最小例子（不写代码也能做）
- 做Lasso路径图；比较Lasso/岭/子集选择在噪声下的稳定性。


### 常见误区
- 同一λ到处复用；忽略标准化与相关特征成组效应。


### 术语小抄
- L0/L1、KKT、LARS、组/融合/弹性网、ARD/SBL。


### 什么时候用（实践场景）
- 特征选择、可解释建模、稀疏感知。


### 读完你应该会
- 稳定求解L1并做超参选择；读懂稀疏解的意义。


### 往后怎么学（书内锚点）
- → Ch14 稀疏向量机；→ Ch26 图Lasso。


## 第 14 章 Kernels & SVMs


### 为什么重要（一句话）
- 非线性建模与相似度工程的主力方案。


### 入门先做什么（Skim / Focus / Skip）
- 理解核=内积；会核岭、SVM分类/回归；知道KDE/KPCA。


### 直觉图景
- 核把数据隐式映到高维线性化；SVM最大间隔更抗过拟合。


### 最小例子（不写代码也能做）
- 在RBF核上网格搜索C/γ；画支持向量与决界。


### 常见误区
- 核/正则不调参；核矩阵病态不加正则。


### 术语小抄
- 正定核、SVM、KDE、KPCA、核回归。


### 什么时候用（实践场景）
- 文本/图像/生物序列等相似度驱动任务。


### 读完你应该会
- 选择核与C；解读支持向量与间隔。


### 往后怎么学（书内锚点）
- → Ch15 与GP对比；→ Ch16 与Boosting对比。


## 第 15 章 Gaussian processes


### 为什么重要（一句话）
- 无需设定特征的函数先验，对小数据极具吸引力。


### 入门先做什么（Skim / Focus / Skip）
- 从一维回归入手；学会优化边际似然与选择核。


### 直觉图景
- 核像‘相关度’，离得近→函数值相近；条件高斯给闭式预测。


### 最小例子（不写代码也能做）
- 对少量观测点做GP回归，观察不确定带；试不同核与超参。


### 常见误区
- 忽视O(n^3)复杂度；核选择过于僵化。


### 术语小抄
- 协方差函数、边际似然、稀疏GP、GP分类。


### 什么时候用（实践场景）
- 小样本回归、贝叶斯优化、校准预测区间。


### 读完你应该会
- 训练/预测并调超参；选近似方法处理大数据。


### 往后怎么学（书内锚点）
- → Ch9 GLM对照；→ Ch14 核方法联系。


## 第 16 章 Adaptive basis / Trees / Boosting / NN


### 为什么重要（一句话）
- 从可解释树到高性能Boosting，再到神经网络的过渡章。


### 入门先做什么（Skim / Focus / Skip）
- 先会CART与剪枝，再学梯度提升；最后过MLP/CNN与反向传播。


### 直觉图景
- 树=递归分割空间；Boosting=加性模型沿负梯度前进；NN=可学习的复合非线性。


### 最小例子（不写代码也能做）
- 用XGBoost/LightGBM做基线；看特征重要度与偏依赖图。


### 常见误区
- 树深过拟合；Boosting学习率/正则缺省；NN未做正则/早停。


### 术语小抄
- CART、随机森林、GAM/MARS、Ada/Logit/梯度提升、反向传播。


### 什么时候用（实践场景）
- 结构化表格数据（树系强势）、高非线性关系、端到端表示。


### 读完你应该会
- 调树/Boosting关键超参；实现简单MLP并正则化。


### 往后怎么学（书内锚点）
- → Ch28 深度生成/自编码；→ Ch6 泛化理论支撑。


## 第 17 章 Markov & HMM


### 为什么重要（一句话）
- 离散时序与序列标注的经典工作马。


### 入门先做什么（Skim / Focus / Skip）
- 会前向/前后向/Viterbi；理解不同推断任务。


### 直觉图景
- 马尔可夫假设=只看最近状态；隐状态生成观测。


### 最小例子（不写代码也能做）
- 构造POS简化HMM，做Viterbi解码；试EM估计转移/发射。


### 常见误区
- 状态数随意设；数值下溢未取log。


### 术语小抄
- 转移矩阵、前向/后向、Viterbi、Baum–Welch。


### 什么时候用（实践场景）
- 语音/文本标注、简单时序分段。


### 读完你应该会
- 完成三类推断与参数学习；做模型选择。


### 往后怎么学（书内锚点）
- → Ch18 连续状态SSM；→ Ch21/23 近似/粒子滤波。


## 第 18 章 State space models


### 为什么重要（一句话）
- 连续状态时序估计与控制的标配模型。


### 入门先做什么（Skim / Focus / Skip）
- 先掌握Kalman滤波/平滑/EM；再看EKF/UKF/ADF。


### 直觉图景
- 预测-更新的循环；线性高斯给闭式，非线性靠近似。


### 最小例子（不写代码也能做）
- 用KF平滑一维位置噪声序列；与ARIMA误差比较。


### 常见误区
- 过程/观测噪声协方差估错；数值稳定性差。


### 术语小抄
- KF/KS、递推最小二乘、EKF/UKF/ADF、混合SSM。


### 什么时候用（实践场景）
- 目标跟踪、导航、传感器融合、时间序列预测。


### 读完你应该会
- 实现KF/KS与EM估参；选择合适近似滤波器。


### 往后怎么学（书内锚点）
- → Ch23 粒子滤波；→ Ch4/15 高斯/核联系。


## 第 19 章 UGMs / CRFs / StructSVMs


### 为什么重要（一句话）
- 能量式/条件式结构化预测的主阵地。


### 入门先做什么（Skim / Focus / Skip）
- 理解势函数与归一化常数；链式CRF训练与解码。


### 直觉图景
- 把‘打分’写成因子之和；CRF直接建p(y|x)。


### 最小例子（不写代码也能做）
- 对序列标注做线性链CRF，与HMM比较F1。


### 常见误区
- 未正则化导致过拟合；配分函数近似不当。


### 术语小抄
- Hammersley–Clifford、伪似然、对比散度、CRF、结构SVM。


### 什么时候用（实践场景）
- 分割/标注/对齐等结构输出任务。


### 读完你应该会
- 训练CRF并做推断；理解StructSVM的间隔式看法。


### 往后怎么学（书内锚点）
- → Ch20–22 推断；→ Ch26 结构学习。


## 第 20 章 Exact inference


### 为什么重要（一句话）
- 明白啥时候能‘算得准’，以及为什么常常算不动。


### 入门先做什么（Skim / Focus / Skip）
- 树上BP、变量消元→结点树；关注树宽与复杂度。


### 直觉图景
- 消息传递像把局部证据汇总；树宽限制一切。


### 最小例子（不写代码也能做）
- 把一个非树图三角化成结点树，跑一次消息传递。


### 常见误区
- 忽视图结构导致指数级计算。


### 术语小抄
- BP、变量消元、结点树、树宽。


### 什么时候用（实践场景）
- 小规模或近树结构的精确推断。


### 读完你应该会
- 为给定图选择合适精确/近似方法。


### 往后怎么学（书内锚点）
- → Ch21 变分；→ Ch22 EP/LBP；→ Ch24 采样。


## 第 21 章 Variational inference I


### 为什么重要（一句话）
- 把难推断转为可优化的下界问题，适配大规模。


### 入门先做什么（Skim / Focus / Skip）
- 学ELBO与均值场；会VB for 高斯/线回归。


### 直觉图景
- 用一个‘可解的分布’逼近后验，并最小化KL。


### 最小例子（不写代码也能做）
- 给混合高斯做VBEM，与EM对比收敛与速度。


### 常见误区
- 不检查近似质量；过度因子化导致欠拟合。


### 术语小抄
- ELBO、均值场、VB/VBEM、局部界。


### 什么时候用（实践场景）
- 大规模Bayes、在线/批量近似。


### 读完你应该会
- 推导均值场更新；评估ELBO收敛。


### 往后怎么学（书内锚点）
- → Ch22 EP/GBPs；→ Ch24 MCMC对照。


## 第 22 章 More VI / EP / MAP


### 为什么重要（一句话）
- 理解EP与环上BP的理论与实践，并掌握MAP优化工具。


### 入门先做什么（Skim / Focus / Skip）
- EP=矩匹配；LBP可能不收敛；图割/LP/双分解用于MAP。


### 直觉图景
- 不同自由能对应不同近似；MAP=寻找最高势配置。


### 最小例子（不写代码也能做）
- TrueSkill小例子：用EP估计玩家技能；与LBP比较。


### 常见误区
- 把EP当保证收敛；忽视高阶结构导致劣解。


### 术语小抄
- 边缘多面体、自由能、EP、GraphCuts、Dual Decomposition。


### 什么时候用（实践场景）
- 排名/匹配/分割的近似推断与MAP求解。


### 读完你应该会
- 实现EP与MAP求解；知道何时该换近似族。


### 往后怎么学（书内锚点）
- → Ch23–24 采样；→ Ch19 结构化任务。


## 第 23 章 Monte Carlo inference


### 为什么重要（一句话）
- 用采样解决积分与时序估计的通用方法。


### 入门先做什么（Skim / Focus / Skip）
- 拒绝/重要性/似然加权；粒子滤波三件套（提议/权重/重采样）。


### 直觉图景
- 更多有效样本→更小方差；提议接近目标更好。


### 最小例子（不写代码也能做）
- 用SIR追踪非线性系统状态；比较不同提议函数。


### 常见误区
- 退化不重采样；方差爆炸不做Rao-Blackwell化。


### 术语小抄
- RS/IS、SIR、退化、Rao–Blackwell。


### 什么时候用（实践场景）
- 非线性非高斯时序、在线估计。


### 读完你应该会
- 设计提议/重采样策略；评估退化程度。


### 往后怎么学（书内锚点）
- → Ch24 MCMC；→ Ch18 KF对照。


## 第 24 章 MCMC


### 为什么重要（一句话）
- 复杂后验的‘通吃’方法库。


### 入门先做什么（Skim / Focus / Skip）
- Gibbs/MH起步；看HMC/切片/辅助变量提升效率；学收敛诊断。


### 直觉图景
- 构造能到处走的链，久而久之样本像来自目标分布。


### 最小例子（不写代码也能做）
- 对Logistic回归做Pólya-Gamma或HMC采样；画轨迹/自相关。


### 常见误区
- 烧入不足；链混合差；不做诊断就报告结果。


### 术语小抄
- Gibbs、MH、HMC、R̂、ESS、并行退火。


### 什么时候用（实践场景）
- 复杂Bayes后验、证据近似、层次模型。


### 读完你应该会
- 正确运行与诊断采样；控制混合与步长。


### 往后怎么学（书内锚点）
- → Ch5 证据；→ Ch21–22 近似对照。


## 第 25 章 Clustering


### 为什么重要（一句话）
- 无监督探索的第一步，帮助形成假设与数据理解。


### 入门先做什么（Skim / Focus / Skip）
- 先学相似度/距离；再学DP混合/谱/层次；会评估指标。


### 直觉图景
- ‘像的在一起’：图方法把数据变成图再分割。


### 最小例子（不写代码也能做）
- 用谱聚类和高斯混合在同数据上比较ARI/NMI。


### 常见误区
- 强行定K；把聚类标签当真值。


### 术语小抄
- 相似度、DP混合、Affinity Propagation、谱、层次/双聚类。


### 什么时候用（实践场景）
- 客户分群、异常检测、探索性分析。


### 读完你应该会
- 选择度量与算法；评估与报告稳定性。


### 往后怎么学（书内锚点）
- → Ch26 结构学习；→ Ch27 主题模型。


## 第 26 章 Graphical model structure learning


### 为什么重要（一句话）
- 让结构从数据里‘长出来’，迈向因果推理。


### 入门先做什么（Skim / Focus / Skip）
- Chow–Liu树、DAG搜索/打分、图Lasso；了解结构EM与因果判定要点。


### 直觉图景
- 相关→边；加上先验/正则避免过拟合；潜变量会诱导虚假边。


### 最小例子（不写代码也能做）
- 对多变量高斯做图Lasso；对离散变量做Chow–Liu树。


### 常见误区
- 把相关当因果；忽略等价类；未处理缺失。


### 术语小抄
- 相关/互信息、DAG等价类、图Lasso、结构EM、因果DAG。


### 什么时候用（实践场景）
- 基因网络、金融/传感网络、可解释模型发现。


### 读完你应该会
- 为目标数据挑选结构学习策略；处理小样本与高维。


### 往后怎么学（书内锚点）
- → Ch10/19 图模型；→ Ch13 稀疏正则。


## 第 27 章 Discrete-data LVMs (LDA/RBM/etc.)


### 为什么重要（一句话）
- 文本/图/关系数据的主力表示学习方法。


### 入门先做什么（Skim / Focus / Skip）
- 先学LDA（Gibbs/VB/Online）；再看SBM/MMSBM/RTM与PMF；了解RBM训练。


### 直觉图景
- 文档是‘主题的混合’，边/交互来自潜社团或潜因子。


### 最小例子（不写代码也能做）
- 在20NG上训练LDA，观察主题词；用PMF做小型推荐实验。


### 常见误区
- 把主题数当越多越好；忽略评估（困惑度/一致性）。


### 术语小抄
- LDA/mPCA/NMF、SBM/MMSBM、RTM、IRM、PMF、RBM。


### 什么时候用（实践场景）
- NLP主题/标签扩展、社群检测、推荐系统。


### 读完你应该会
- 拟合/评估主题模型；选择主题数；做简单关系建模。


### 往后怎么学（书内锚点）
- → Ch21–24 推断；→ Ch12 连续潜因子对照。


## 第 28 章 Deep learning


### 为什么重要（一句话）
- 学习深层表示与早期深度生成模型的历史视角。


### 入门先做什么（Skim / Focus / Skip）
- 了解DBN/DBM/自编码器/贪婪预训练；与现代端到端训练做对比。


### 直觉图景
- 逐层学‘好特征’，再微调；自编码器学重构、DBN学生成。


### 最小例子（不写代码也能做）
- 做一个去噪自编码器；用栈式AE做降维可视化。


### 常见误区
- 把预训练当万能；忽略正则/早停。


### 术语小抄
- DBN/DBM、Auto-Encoder、SAE、卷积、语义哈希。


### 什么时候用（实践场景）
- 特征学习、可视化、检索/哈希。


### 读完你应该会
- 实现AE/深MLP；理解预训练–微调范式。


### 往后怎么学（书内锚点）
- → Ch16 现代NN/集成；与当代深度学习框架衔接。
