# MLAPP 新手友好指南 · v3（每章“益理解版”总结）

> 每章仅保留一段话，帮助你先建立直觉，再决定是否细读该章其它部分。


## 第 1 章 Introduction

本章像是地图的图例：先告诉你机器学习分为监督/无监督等类型，再用 KNN、线性/逻辑回归这些小例子演示“数据→模型→评估”的节奏。你可以把模型当成‘生成数据的故事’，学习就是根据观测去反推这个故事的参数。过拟合、模型选择和维数灾难这些词，都是在提醒你：好成绩不等于好泛化。读完后，你知道后面每章在大地图上的位置。


## 第 2 章 Probability

把概率想成‘相信程度’的刻度尺，分布的形状表达你对世界的想象；条件独立允许把大难题拆成小块。信息论量化“惊不惊讶”（熵）、“把 q 当 p 的代价”（KL）和“X 告诉了 Y 多少”（互信息）。蒙特卡罗就像多次掷骰，用样本近似难以计算的期望。读完后，后面的每个公式都能找到语义。


## 第 3 章 Generative models (discrete)

离散生成式建模像在讲‘数硬币/数词’的故事：Beta–Binomial 和 Dirichlet–Multinomial 这两对共轭组合，让先验+数据得到后验，还能直接给预测。朴素贝叶斯把“每个词条件独立”这种强假设换成超快、鲁棒的分类器；平滑像“给没见过的词留点面子”。读完后，你能用最少的计算做出可解释的离散分类与估计。


## 第 4 章 Gaussian models

多元高斯可以想成椭圆云：协方差决定云的拉伸和方向；条件/边缘后仍是高斯，计算方便。判别分析（LDA/QDA）把类条件高斯接到 Bayes 规则上，得到线性/二次边界。精度矩阵的稀疏性还直接对应条件独立。读完后，你会把‘高斯’当成解析可解的第一选择。


## 第 5 章 Bayesian statistics

贝叶斯的核心是把‘不知道’量化：参数有分布，结论带不确定性。证据（边际似然）倾向简单模型，像内置的 Occam 剃刀；层次先验让相似任务‘抱团取暖’。贝叶斯决策把损失放进来，关注“错了的代价”。读完后，你知道何时该让模型“说我不确定”。


## 第 6 章 Frequentist statistics

频率学派关心“如果重复抽样会发生什么”：因此重视偏差—方差、置信区间与交叉验证。ERM/SRM 让我们在训练误差和模型复杂度之间找平衡，Bootstrap 用来量化不确定性。也提醒你别滥用 p 值、别误解置信区间。读完后，你能更可靠地比较模型并报告结果。


## 第 7 章 Linear regression

线性回归可以看成‘把点投影到一条线或一个子空间上’。岭回归像给系数上弹簧，避免它们乱跑；稳健回归防极端点捣乱。换到贝叶斯视角，就是给系数加高斯先验。读完后，你能稳稳当当搭建一个可解释、可诊断的回归基线。


## 第 8 章 Logistic regression

逻辑回归把‘属于这一类的对数几率’建成线性函数，所以能用凸优化稳稳地解；Softmax 是它的多类版。IRLS/Newton 速度快，SGD 更适合大数据与在线。正则化控制复杂度，阈值调节则控制精/召权衡。读完后，你拥有一个工业级分类起跑器。


## 第 9 章 GLMs & Exponential family

GLM 就像“选分布 + 选链接”的配方：用指数族描述噪声特性，再用链接函数把线性预测映回目标域。这样，二项、泊松、序数等问题能在同一框架里解决。读完后，你知道遇到‘次数、比例、等级’等输出类型时该选哪一道菜。


## 第 10 章 Directed graphical models

有向图模型是“会计分录版”的概率建模：因子分解清楚、依赖方向明确。d-分离告诉你哪些变量独立，从而可以“少算很多”。板式记号像循环结构的缩写。读完后，你能画出一个能说清假设、能落地推断的 Bayes 网。


## 第 11 章 Mixture models & EM

混合模型把“数据来自若干子人群”的直觉形式化；EM 像‘先软分配，再重估’的两步舞：E 步猜隐变量，M 步更新参数。它会收敛到一个合理的局部最优，因此初始化与模型阶数很关键。读完后，你会用 EM 驾驭含隐变量的问题。


## 第 12 章 Latent linear models (FA/PCA/ICA)

PCA/FA/ICA 分别回答‘主要方向’、‘低维因子’和‘独立源’三个问题。把数据看作在低维子空间上的投影，或由少数因子加噪声生成，就能降维、去噪和可视化；ICA 依靠非高斯性分离混合信号。读完后，你会用一张图解释“高维数据为何其实很低秩”。


## 第 13 章 Sparse linear models

稀疏化让模型‘只保留真正有用的特征’：L1 正则像软阈值，直接把小系数压到 0；ARD 用先验自动“关掉”无用权重。路径图展示正则强度变化时哪些特征最稳定。读完后，你能在高维小样本里既保命（泛化）又要脸（解释）。


## 第 14 章 Kernels & SVMs

核方法把‘相似度’变成一等公民：只要定义好内积，就能在隐式高维里线性地做事。SVM 倡导最大间隔，像“留足缓冲区以防过拟合”。KDE/KPCA/核回归把核用在生成式与降维上。读完后，你能围绕“合适的核+合适的正则”搭一套非线性方案。


## 第 15 章 Gaussian processes

高斯过程把‘对函数的相信’直接写成协方差：核决定我们认为函数该多平滑、多周期。条件高斯给出带不确定度的预测带，超参由边际似然来学。数据一多就贵，所以要用稀疏/近似。读完后，你能把‘我不确定’体现在预测区间里。


## 第 16 章 Adaptive basis / Trees / Boosting / NN

树把空间划成一小块一小块，很直观；Boosting 则像‘每次修正上一次的错误’，逐步叠加弱学习器；神经网络让特征本身可学习。这三者覆盖‘可解释—高性能—端到端’三角。读完后，你能在表格、非线性、端到端之间做权衡。


## 第 17 章 Markov & HMM

马尔可夫链只看最近一步的记忆；HMM 说‘状态看不见，但会产生观测’。三种推断（过滤/平滑/解码）回答“现在/过去/最可能路径”。前向/后向/Viterbi 是三板斧。读完后，你能把一长串序列拆成可处理的局部计算。


## 第 18 章 State space models

状态空间模型把系统分成‘状态演化’和‘观测生成’两条线：线性高斯时，卡尔曼滤波/平滑给出闭式的预测-更新；非线性时，用 EKF/UKF/ADF 做近似。读完后，你能把噪声很大的时序信号‘稳住’。


## 第 19 章 UGMs / CRFs / StructSVMs

无向图模型强调‘打分’而非‘因果方向’：势函数累加成能量，配分函数完成归一化。CRF 直接建 p(y|x)，避免了一些生成式的偏置；结构 SVM 从间隔角度做结构输出。读完后，你知道何时该选“能量式”的表达。


## 第 20 章 Exact inference

精确推断依赖图的‘树宽’：树上可以消息传递，圈多了就要用结点树或消元，复杂度会飙升。本章让你明白‘为什么很多时候必须放弃精确’。读完后，你能看图识难度，决定是否要上近似。


## 第 21 章 Variational inference I

变分推断把‘难的后验’换成‘容易的近似分布’，通过最大化 ELBO 去拟合。均值场像‘各管各的’的妥协，计算快但可能欠拟合；结构化均值场更灵活。读完后，你把推断问题也当作一个优化问题来对待。


## 第 22 章 More VI / EP / MAP

本章把近似推断的谱系讲全：循环 BP 的自由能视角、广义/凸 BP 的改进，期望传播（EP）用矩匹配来“让每块都像”。随后切换到 MAP：图割、LP 松弛与双分解都是找“最高得分配置”的优化器。读完后，你能在‘近似边缘’与‘近似最优状态’间自如选择。


## 第 23 章 Monte Carlo inference

蒙特卡罗的核心是‘用样本代表分布’：拒绝/重要性采样解决静态积分；粒子滤波把它搬到时间轴上，通过‘采样→加权→重采样’实时更新。关键在于选好提议、控制方差。读完后，你会让随机性来替你算难题。


## 第 24 章 MCMC

MCMC 通过构造一条‘长期来看像目标分布’的随机游走来采样：Gibbs 和 MH 是基础，HMC/切片/辅助变量让走路更聪明。收敛诊断（R̂/ESS）确保我们“真的到了”。读完后，你能从复杂后验里抽样并知道何时可以信任它。


## 第 25 章 Clustering

聚类先定‘像不像’的度量，再挑算法：DP 混合（生成式）、谱聚类（图视角）、层次聚类（自上/下）各有侧重。评估与稳定性很重要，别迷信可视化。读完后，你能用“度量+评估”组织无监督探索。


## 第 26 章 Graphical model structure learning

结构学习让‘网络关系’从数据里长出来：相关/互信息造出树（Chow–Liu），更复杂的 DAG 需要搜索与打分；潜变量会制造假相关，需要边际似然或结构 EM 处理。因果 DAG 给了解释与反事实的通道。读完后，你知道如何从零开始发现结构。


## 第 27 章 Discrete-data LVMs (LDA/RBM/etc.)

离散潜变量模型把‘主题/社团/关系’变成可学习的隐变量：LDA 把文档看作主题混合，SBM 给图找社群，PMF 给推荐找潜因子，RBM 作为能量式编码器。推断可用 Gibbs/VB/在线 VI。读完后，你能为离散世界学到有意义的低维表示。


## 第 28 章 Deep learning

本章以早期深度学习为主角：逐层预训练的 DBN/DBM、去噪自编码器与卷积网络展示“学特征→再微调”的策略。它补全了从浅层到深层表示学习的谱系，并给出图像/语音/检索等早期应用。读完后，你能把‘特征工程’更多交给模型本身。
